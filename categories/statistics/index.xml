<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Statistics on Lluc Puig Codina</title>
    <link>https://llucpuigcodina.github.io/categories/statistics/</link>
    <description>Recent content in Statistics on Lluc Puig Codina</description>
    <generator>Hugo -- gohugo.io</generator>
    <lastBuildDate>Sat, 13 Dec 2025 00:00:00 +0000</lastBuildDate><atom:link href="https://llucpuigcodina.github.io/categories/statistics/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Markov Chains and Smoothing</title>
      <link>https://llucpuigcodina.github.io/posts/markov_chain_smoothing/</link>
      <pubDate>Sat, 13 Dec 2025 00:00:00 +0000</pubDate>
      
      <guid>https://llucpuigcodina.github.io/posts/markov_chain_smoothing/</guid>
      <description>body {text-align: justify}Let \(\left\{ X_t \right\}_{t \geq 1}\) be a Markov chain. Use notation \(x_{i:j} = \left(x_i, x_{i+1}, \dots, x_j \right)\). By the Markov property,
\[\begin{align*}\pi\left(x_{1:T}\right) &amp;amp;= \pi\left(x_{k+1:T} | x_{1:k+1} \right)\pi\left(x_{k+1}|x_{1:k} \right)\pi\left(x_k | x_{1:k-1} \right)\pi\left(x_{1:k-1}\right)\\&amp;amp;= \pi\left(x_{k+1:T} | x_{k+1} \right)\pi\left(x_{k+1}|x_{k} \right)\pi\left(x_k | x_{k-1} \right)\pi\left(x_{1:k-1}\right)\end{align*}\]
Then,
\[\begin{align*}\pi\left(x_k | x_{1:k-1}, x_{k+1:T}\right) &amp;amp;= \frac{\pi\left(x_{1:T}\right)}{\pi\left(x_{1:k-1}, x_{k+1:T} \right)}\\&amp;amp;= \frac{\pi\left(x_{1:T}\right)}{\int\pi\left(x_{1:T}\right) dx_k}\\&amp;amp;= \frac{\pi\left(x_{k+1:T} | x_{k+1} \right)\pi\left(x_{k+1}|x_{k} \right)\pi\left(x_k | x_{k-1} \right)\pi\left(x_{1:k-1}\right)}{\int \pi\left(x_{k+1:T} | x_{k+1} \right)\pi\left(x_{k+1}|x_{k} \right)\pi\left(x_k | x_{k-1} \right)\pi\left(x_{1:k-1}\right) dx_k} \\&amp;amp;= \frac{\pi\left(x_{k+1:T} | x_{k+1} \right)\pi\left(x_{k+1}|x_{k} \right)\pi\left(x_k | x_{k-1} \right)\pi\left(x_{1:k-1}\right)}{\pi\left(x_{k+1:T} | x_{k+1} \right) \left[ \int \pi\left(x_{k+1}|x_{k} \right)\pi\left(x_k | x_{k-1} \right) dx_k \right] \pi\left(x_{1:k-1}\right) } \\&amp;amp;= \frac{\pi\left(x_{k+1}|x_{k} \right)\pi\left(x_k | x_{k-1} \right)}{\int \pi\left(x_{k+1}|x_{k} \right)\pi\left(x_k | x_{k-1} \right) dx_k} \\&amp;amp;= \frac{\pi\left(x_{k+1}, x_k | x_{k-1} \right)}{\pi\left( x_{k+1} | x_{k-1} \right)}\\&amp;amp;= \pi\left( x_k | x_{k-1}, x_{k+1} \right)\end{align*}\]Given all past and future, all information about \(x_k\) is contained in it’s nearest neighbors \(x_{k-1}\) and \(x_{k+1}\).</description>
    </item>
    
    <item>
      <title>On the limiting behaviour of the Uhlig 1997 Stochastic Volatility model</title>
      <link>https://llucpuigcodina.github.io/posts/uhligstochvol/</link>
      <pubDate>Wed, 09 Oct 2024 00:00:00 +0000</pubDate>
      
      <guid>https://llucpuigcodina.github.io/posts/uhligstochvol/</guid>
      <description>body {text-align: justify}Uhlig (1997) proposed a VAR with stochastic volatility where precision, the inverse of variance, is given by a multiplicative process with beta innovations. The model lends itself to a high level of tractability since exact updating formulas are obtained. Lately I’ve been playing around with this model and found out some interesting aspects.
For the simple scalar case the model is as follows:</description>
    </item>
    
    <item>
      <title>Drawing with Restrictions in MCMC</title>
      <link>https://llucpuigcodina.github.io/posts/mhrestrictiongeweke/</link>
      <pubDate>Tue, 18 Jun 2024 00:00:00 +0000</pubDate>
      
      <guid>https://llucpuigcodina.github.io/posts/mhrestrictiongeweke/</guid>
      <description>body {text-align: justify}It is often the case that, when simulating from posterior distributions using MCMC, researchers wish to impose restrictions on some parameters.
Two illustrative cases are discussed in Geweke’s Contemporary Bayesian Econometrics and Statistics Example 4.2.2 and 4.3.2 . The following description of the setup (and notation) is taken almost verbatim from Geweke. These examples consider a normal linear regression model with Gaussian disturbances in which the prior of \(\boldsymbol{\beta}\) (the regression coefficient) given \(h\) (precision) is \(\mathscr{N}(\underline{\boldsymbol{\beta}}, \underline{\textbf{H}})\) truncated to a set \(S\subseteq \mathbb{R}^k\).</description>
    </item>
    
    <item>
      <title>X &gt; MAX for independent but not identically distributed betas</title>
      <link>https://llucpuigcodina.github.io/posts/abtest_general_formula/</link>
      <pubDate>Fri, 13 Oct 2023 00:00:00 +0000</pubDate>
      
      <guid>https://llucpuigcodina.github.io/posts/abtest_general_formula/</guid>
      <description>body {text-align: justify}In this post I provide a general formula for the probability that the realization of a beta random variable is larger than the realizations from a sequence of beta random variables, all of them being independent. Assume \(p_j \sim \mathscr{B}(\alpha_j, \beta_j)\). We have J+1 independent but not identically distributed beta random variables, indexed from 0 to J, and we are interested in calculating \(\mathbb{P}(p_0 &amp;gt; max\{p_1,\dots,p_J\})\).</description>
    </item>
    
  </channel>
</rss>
